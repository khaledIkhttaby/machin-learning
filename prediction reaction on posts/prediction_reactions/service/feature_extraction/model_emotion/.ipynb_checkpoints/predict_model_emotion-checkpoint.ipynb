{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.models import Sequential,load_model\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def split_emoji(txt):\n",
    "    x=' '.join(c for c in txt.split() if c not in emoji.UNICODE_EMOJI)\n",
    "    return x\n",
    "#     em = txt\n",
    "#     em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "#     em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "#     em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "#     res = ''\n",
    "\n",
    "#     for i in range(len(em_split)):\n",
    "#         if em_split[i] not in emoji.UNICODE_EMOJI:\n",
    "#             if i != len(em_split) - 1:\n",
    "#                 res += em_split[i] + ' '\n",
    "#             else:\n",
    "#                 res += em_split[i]\n",
    "#         else:\n",
    "#             continue\n",
    "#     return res\n",
    "\n",
    "stop_word=pd.read_csv(\"stopwords.csv\",names=['data'])['data'].values\n",
    "i=0\n",
    "def len_sent(text):\n",
    "    return len(text.split())\n",
    "def preprocessing(data ):\n",
    "    data=str(data)\n",
    "    data = split_emoji(data)\n",
    "    data = str(' '.join(re.sub(\"([٠١٢٣٤٥٦٧٨٩]+)|([0-9]+)|([A-Za-z]+)|\\_+|(\\#)+|(\\/)+|(\\:)+\", \" \", data).split()))\n",
    "    data = re.sub(\"[إأٱآا]\", \"ا\", data)\n",
    "    data = re.sub(\"ة\", \"ه\", data)\n",
    "    # remove duplicate\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                            َ    | # Fatha\n",
    "                            ً    | # Tanwin Fath\n",
    "                            ُ    | # Damma\n",
    "                            ٌ    | # Tanwin Damm\n",
    "                            ِ    | # Kasra\n",
    "                            ٍ    | # Tanwin Kasr\n",
    "                            ْ    | # Sukun\n",
    "                            ـ   |  # Tatwil/Kashida\n",
    "                            ،   |\n",
    "                        \"\"\", re.VERBOSE)\n",
    "    flagsUs = re.compile(\"[\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "    dirtyChars = re.compile(\"[\"\n",
    "                            \"\\u0600-\\u0620\"\n",
    "                            \"\\u063B-\\u0640\"\n",
    "                            \"\\u064B-\\u065F\"\n",
    "                            \"\\u066A-\\u06FF\"\n",
    "                            \"\\u0750-\\u077F\"\n",
    "                            \"\\u08A0-\\u08FF\"\n",
    "                            \"\\uFB50-\\uFBE9\"\n",
    "                            \"\\uFBF0-\\uFBFB\"\n",
    "                            \"\\uFC5B-\\uFC63\"\n",
    "                            \"\\uFCF2-\\uFCF4\"\n",
    "                            \"\\uFD3C-\\uFD4F\"\n",
    "                            \"\\uFD90-\\uFD91\"\n",
    "                            \"\\uFDC8-\\uFDFF\"\n",
    "                            \"\\uFE70-\\uFE7F\"\n",
    "                            \"\\uFEFD-\\uFEFF\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    data = str(re.sub(flagsUs, '', data))\n",
    "    data = str(re.sub(dirtyChars, '', data))\n",
    "    data = str(re.sub(noise, '', data))\n",
    "#     data = str(''.join(c for c in data if c not in punctuation))\n",
    "    data = re.sub(\n",
    "        '\\/+|\\●+|\\◽+|\\٪+|\\▪+|\\»+|\\«+|\\_+|\\ʚïɞ+|\\▐+|\\►+|\\\"+|\\*+|\\▁+|\\》+|\\《+|\\[+|\\Ещё+|\\]+|\\|+|\\;+|\\'+|\\<+|\\>+|\\\\+|\\`+|\\{+|\\}+|\\~+|\\\"+|\\-+|\\:+|\\@+|\\#+|\\$+|\\ﷺ+|\\%+|\\^+|\\&+|\\(+|\\)+|\\.+|\\,+|\\?+|\\=+|\\++|\\؛+\\“+|\\”+',\n",
    "        ' ', data)\n",
    "    data = re.sub('\\!', '  ', data)\n",
    "    data = re.sub('\\⚘', '  ', data)\n",
    "    data = re.sub('\\��', '  ', data)\n",
    "    data = re.sub('\\؟ ', ' ', data)\n",
    "    data = re.sub('\\.', ' ', data)\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    data = re.sub('\\\\\\+', ' ', data)\n",
    "    data = re.sub('\\\"+',' ',data)\n",
    "    data = re.sub(' ا ', ' ', data)\n",
    "    data = re.sub(' ب ', ' ', data)\n",
    "    data = re.sub(' ت ', ' ', data)\n",
    "    data = re.sub(' ث ', ' ', data)\n",
    "    data = re.sub(' ج ', ' ', data)\n",
    "    data = re.sub(' ح ', ' ', data)\n",
    "    data = re.sub(' خ ', ' ', data)\n",
    "    data = re.sub(' د ', ' ', data)\n",
    "    data = re.sub(' ذ ', ' ', data)\n",
    "    data = re.sub(' ر ', ' ', data)\n",
    "    data = re.sub(' ز ', ' ', data)\n",
    "    data = re.sub(' س ', ' ', data)\n",
    "    data = re.sub(' ش ', ' ', data)\n",
    "    data = re.sub(' ص ', ' ', data)\n",
    "    data = re.sub(' ض ', ' ', data)\n",
    "    data = re.sub(' ط ', ' ', data)\n",
    "    data = re.sub(' ظ ', ' ', data)\n",
    "    data = re.sub(' ع ', ' ', data)\n",
    "    data = re.sub(' غ ', ' ', data)\n",
    "    data = re.sub(' ف ', ' ', data)\n",
    "    data = re.sub(' ق ', ' ', data)\n",
    "    data = re.sub(' ك ', ' ', data)\n",
    "    data = re.sub(' ل ', ' ', data)\n",
    "    data = re.sub(' م ', ' ', data)\n",
    "    data = re.sub(' ن ', ' ', data)\n",
    "    data = re.sub(' ه ', ' ', data)\n",
    "    data = re.sub(' و ', ' ', data)\n",
    "    data = re.sub(' ي ', ' ', data)\n",
    "    data = re.sub(' ئ ', ' ', data)\n",
    "    data = re.sub(' ؤ ', ' ', data)\n",
    "    data = re.sub(' ء ', ' ', data)\n",
    "    data = re.sub('\\_+', ' ', data)\n",
    "    data = re.sub('\\…+', ' ', data)\n",
    "    data = re.sub('\\“|\\”', '', data)\n",
    "    data = re.sub(r'([\\u0600-\\u06FF])\\1{3,}', r'\\1\\1\\1', data)\n",
    "    data = re.sub(r'[\\u2066]', ' ', data)\n",
    "    data = re.sub(r'[\\u2069]', ' ', data)\n",
    "    data = re.sub(r'[\\uFE0F]', ' ', data)\n",
    "    data = re.sub(r'[\\u25a0]', ' ', data)\n",
    "    data = re.sub(r'[\\u2022]', ' ', data)\n",
    "    data = re.sub(r'[\\u2592]', ' ', data)\n",
    "    data = re.sub('[\\u1ea0]|[\\u1ea1]|[\\u1e97]|[\\u1ea1]|[\\u02bf]|[\\u1e97]|[\\u1ea1]|[\\u1e97]|[\\u1ea1]|[\\u1ea1]|[\\u02be]|[\\u1ea1]|[\\u1ea1]',' ',data)\n",
    "    data = ' '.join([word for word in data.split() if word not in stop_word])\n",
    "    data = \" \".join(data.split())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"model_emotion.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_emotion=pickle.load(open(\"tokenizer_model_emotion.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_emotion=pickle.load(open(\"encoder_model_emotion.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_from_model_emotion(sentence):\n",
    "    \n",
    "#     sentence=preprocessing(sentence)\n",
    "    sentence=pd.DataFrame(sentence)[0].apply(preprocessing).values\n",
    "    X_test=tokenizer_model_emotion.texts_to_sequences(sentence)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(sentence):\n",
    "    sentence=pd.DataFrame(sentence)[0].apply(preprocessing).values\n",
    "\n",
    "    X_test=tokenizer_model_emotion.texts_to_sequences(sentence)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
    "    return encoder_model_emotion.inverse_transform([np.argmax(model.predict(X_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_vector_from_model_emotion([\" بكون جبل يهرب علي السجن\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_emotion([\" بكون جبل يهرب علي السجن\",\" بكون جبل يهرب علي السجن\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type([\" بكون جبل يهرب علي السجن\"])==list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([\" بكون جبل يهرب علي السجن\",\"بكون جبل يهرب علي السجن\"])[0].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
